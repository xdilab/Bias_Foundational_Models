{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "hXle99u2-SxD",
        "outputId": "037a301f-e263-4526-e74e-d179a5a098a2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "'''\n",
        "#colab only\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch albumentations torchmetrics kaggle pandas sklearn\n"
      ],
      "metadata": {
        "id": "4y8ZAR1Z1iOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# Install dependencies\n",
        "# =====================\n",
        "!pip install scikit-learn\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from segmentation_models_pytorch import DeepLabV3Plus\n",
        "from segmentation_models_pytorch.metrics import get_stats, iou_score, f1_score\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "metadata": {
        "id": "72__3K6pwd87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce2eb9a3-7496-45ee-87c8-8685fda66fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\money\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\money\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
            "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\money\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\money\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\money\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#COLAB ONLY\n",
        "#Download dataset from kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d mateuszbuda/lgg-mri-segmentation\n",
        "!unzip -q lgg-mri-segmentation.zip -d /content/dataset\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvIUVgiIwg-w",
        "outputId": "1dee2501-b951-44ad-b04c-7445ce5269fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mateuszbuda/lgg-mri-segmentation\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading lgg-mri-segmentation.zip to /content\n",
            " 99% 705M/714M [00:02<00:00, 168MB/s]\n",
            "100% 714M/714M [00:02<00:00, 258MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def LoadData (path1, path2):\n",
        "    \"\"\"\n",
        "    Looks for relevant filenames in the shared path\n",
        "    Returns 2 lists for original and masked files respectively\n",
        "\n",
        "    \"\"\"\n",
        "    # Read the images folder like a list\n",
        "\n",
        "    # Make a list for images and masks filenames\n",
        "    orig_img = []\n",
        "    mask_img = []\n",
        "    for file in path1:\n",
        "        orig_img.append(file)\n",
        "    for file in path2:\n",
        "        mask_img.append(file)\n",
        "\n",
        "    # Sort the lists to get both of them in same order (the dataset has exactly the same name for images and corresponding masks)\n",
        "    orig_img.sort()\n",
        "    mask_img.sort()\n",
        "\n",
        "    return orig_img, mask_img\n"
      ],
      "metadata": {
        "id": "eSQrxkYUa5-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PreprocessData(img, mask, target_shape_img, target_shape_mask, path1, path2, n_slices=3):\n",
        "    \"\"\"\n",
        "    Processes the images and mask present in the shared list and path.\n",
        "    Each input sample includes `n_slices` adjacent slices (center Â±N).\n",
        "    Returns:\n",
        "        X - Image stack of shape (m, H, W, n_slices)\n",
        "        y - Corresponding masks of shape (m, H, W, 1)\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from PIL import Image\n",
        "    import os\n",
        "\n",
        "    m = len(img)  # number of total slices\n",
        "    i_h, i_w, _ = target_shape_img\n",
        "    m_h, m_w, m_c = target_shape_mask\n",
        "\n",
        "    # Image shape now includes n_slices as channels\n",
        "    X = np.zeros((m, i_h, i_w, n_slices), dtype=np.float32)\n",
        "    y = np.zeros((m, m_h, m_w, m_c), dtype=np.float32)\n",
        "\n",
        "    half = n_slices // 2\n",
        "\n",
        "    for index in range(m):\n",
        "        slice_stack = []\n",
        "\n",
        "        for offset in range(-half, half + 1):\n",
        "            slice_idx = index + offset\n",
        "            # Handle boundaries by clamping the index\n",
        "            slice_idx = max(0, min(slice_idx, m - 1))\n",
        "            adj_file = img[slice_idx]\n",
        "            adj_path = os.path.join(path1[slice_idx], adj_file)\n",
        "\n",
        "            adj_img = Image.open(adj_path).convert('L')  # grayscale\n",
        "            adj_img = adj_img.resize((i_h, i_w))\n",
        "            adj_img = np.array(adj_img, dtype=np.float32) / 255.0\n",
        "            slice_stack.append(adj_img)\n",
        "\n",
        "        # Stack into (H, W, n_slices)\n",
        "        stacked_img = np.stack(slice_stack, axis=-1)\n",
        "        X[index] = stacked_img\n",
        "\n",
        "        # Load and process mask (same as original)\n",
        "        mask_file = mask[index]\n",
        "        mask_path = os.path.join(path2[index], mask_file)\n",
        "        single_mask = Image.open(mask_path)\n",
        "        single_mask = single_mask.resize((m_h, m_w))\n",
        "        single_mask = np.array(single_mask, dtype=np.float32)\n",
        "        single_mask[single_mask == 255] = 1\n",
        "        single_mask[single_mask > 1] = 1\n",
        "        single_mask = np.reshape(single_mask, (m_h, m_w, m_c))\n",
        "        y[index] = single_mask\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "PRf5Z3kia2gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_row(path):\n",
        "    \"\"\"Produces ID of a patient, image and mask filenames from a particular path\"\"\"\n",
        "    path_no_ext, ext = os.path.splitext(path)\n",
        "    filename = os.path.basename(path)\n",
        "\n",
        "    patient_id = '_'.join(filename.split('_')[:3]) # Patient ID in the csv file consists of 3 first filename segments\n",
        "\n",
        "    return [patient_id, path, f'{path_no_ext}_mask{ext}']\n",
        "\n",
        "files_dir = '/Users/money/Downloads/archive/kaggle_3m'\n",
        "file_paths = glob(f'{files_dir}/*/*[0-9].tif')\n",
        "train_df = pd.DataFrame((get_file_row(filename) for filename in file_paths), columns=['Patient', 'image_filename', 'mask_filename'])\n",
        "print(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma-YxUVnarjC",
        "outputId": "979cdcb2-6485-4a2a-ad9c-257535b6f916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Patient                                     image_filename  \\\n",
            "0     TCGA_CS_4941  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "1     TCGA_CS_4941  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "2     TCGA_CS_4941  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "3     TCGA_CS_4941  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "4     TCGA_CS_4941  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "...            ...                                                ...   \n",
            "3924  TCGA_HT_A61B  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "3925  TCGA_HT_A61B  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "3926  TCGA_HT_A61B  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "3927  TCGA_HT_A61B  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "3928  TCGA_HT_A61B  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "\n",
            "                                          mask_filename  \n",
            "0     /Users/money/Downloads/archive/kaggle_3m\\TCGA_...  \n",
            "1     /Users/money/Downloads/archive/kaggle_3m\\TCGA_...  \n",
            "2     /Users/money/Downloads/archive/kaggle_3m\\TCGA_...  \n",
            "3     /Users/money/Downloads/archive/kaggle_3m\\TCGA_...  \n",
            "4     /Users/money/Downloads/archive/kaggle_3m\\TCGA_...  \n",
            "...                                                 ...  \n",
            "3924  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...  \n",
            "3925  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...  \n",
            "3926  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...  \n",
            "3927  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...  \n",
            "3928  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...  \n",
            "\n",
            "[3929 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patient_info = pd.read_csv('/Users/money/Downloads/archive/kaggle_3m/data.csv')\n",
        "\n",
        "train_df['Patient'] = train_df['image_filename'].apply(lambda x: os.path.basename(x).split('_')[0] + '_' + os.path.basename(x).split('_')[1] + '_' + os.path.basename(x).split('_')[2])\n",
        "\n",
        "# Map race to each filename\n",
        "#map = dict(zip(patient_info['Patient'], patient_info['race']))\n",
        "#train_df['race'] = train_df['Patient'].map(map)\n",
        "\n",
        "train_df = train_df.merge(\n",
        "    patient_info[['Patient', 'race', 'gender']],\n",
        "    on='Patient',\n",
        "    how='left'\n",
        ")\n",
        "print(train_df)"
      ],
      "metadata": {
        "id": "QaXMwXeIaqaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71dd0cbc-63a1-4d1e-f225-d99eba34f035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Patient                                     image_filename  \\\n",
            "0     TCGA_CS_4941  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "1     TCGA_CS_4941  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "2     TCGA_CS_4941  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "3     TCGA_CS_4941  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "4     TCGA_CS_4941  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "...            ...                                                ...   \n",
            "3924  TCGA_HT_A61B  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "3925  TCGA_HT_A61B  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "3926  TCGA_HT_A61B  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "3927  TCGA_HT_A61B  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "3928  TCGA_HT_A61B  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   \n",
            "\n",
            "                                          mask_filename  race  gender  \n",
            "0     /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   3.0     2.0  \n",
            "1     /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   3.0     2.0  \n",
            "2     /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   3.0     2.0  \n",
            "3     /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   3.0     2.0  \n",
            "4     /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   3.0     2.0  \n",
            "...                                                 ...   ...     ...  \n",
            "3924  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   NaN     NaN  \n",
            "3925  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   NaN     NaN  \n",
            "3926  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   NaN     NaN  \n",
            "3927  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   NaN     NaN  \n",
            "3928  /Users/money/Downloads/archive/kaggle_3m\\TCGA_...   NaN     NaN  \n",
            "\n",
            "[3929 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path1 = train_df[\"image_filename\"].tolist()\n",
        "path2 = train_df[\"mask_filename\"].tolist()\n",
        "img, mask = LoadData (path1, path2)\n",
        "# Define the desired shape\n",
        "target_shape_img = [256, 256, 3]\n",
        "target_shape_mask = [256, 256, 1]\n",
        "\n",
        "# Process data using apt helper function\n",
        "X, y = PreprocessData(img, mask, target_shape_img, target_shape_mask, path1, path2)\n",
        "patients = train_df[\"Patient\"].unique()\n",
        "\n",
        "train_patients, val_patients = train_test_split(\n",
        "    patients, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_df_split = train_df[train_df[\"Patient\"].isin(train_patients)]\n",
        "val_df_split = train_df[train_df[\"Patient\"].isin(val_patients)]\n",
        "\n",
        "# Prepare file lists\n",
        "train_images, train_masks = LoadData(\n",
        "    train_df_split[\"image_filename\"].tolist(),\n",
        "    train_df_split[\"mask_filename\"].tolist()\n",
        ")\n",
        "val_images, val_masks = LoadData(\n",
        "    val_df_split[\"image_filename\"].tolist(),\n",
        "    val_df_split[\"mask_filename\"].tolist()\n",
        ")\n",
        "\n",
        "# Process into arrays with new 256x256 size\n",
        "X_train, y_train = PreprocessData(\n",
        "    train_images, train_masks,\n",
        "    target_shape_img, target_shape_mask,\n",
        "    train_df_split[\"image_filename\"].tolist(),\n",
        "    train_df_split[\"mask_filename\"].tolist()\n",
        ")\n",
        "\n",
        "X_valid, y_valid = PreprocessData(\n",
        "    val_images, val_masks,\n",
        "    target_shape_img, target_shape_mask,\n",
        "    val_df_split[\"image_filename\"].tolist(),\n",
        "    val_df_split[\"mask_filename\"].tolist()\n",
        ")"
      ],
      "metadata": {
        "id": "IY1i2TB2ab94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, images, masks, augment=None):\n",
        "        self.images = images  # already numpy arrays\n",
        "        self.masks = masks    # already numpy arrays\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]   # shape [H,W,C]\n",
        "        mask = self.masks[idx]     # shape [H,W,1] or [H,W]\n",
        "\n",
        "        # Ensure correct types\n",
        "        image = image.astype(np.float32)\n",
        "        mask = mask.astype(np.float32)\n",
        "        mask = (mask > 0.5).astype(np.float32)  # binarize\n",
        "\n",
        "        # Albumentations expects HWC\n",
        "        if self.augment:\n",
        "            augmented = self.augment(image=image, mask=mask)\n",
        "            image = augmented[\"image\"]\n",
        "            mask = augmented[\"mask\"]\n",
        "\n",
        "        # Convert to tensors\n",
        "        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)  # [C,H,W]\n",
        "        if mask.ndim == 2:  # [H,W]\n",
        "            mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)    # [1,H,W]\n",
        "        else:  # [H,W,1]\n",
        "            mask = torch.tensor(mask, dtype=torch.float32).permute(2, 0, 1)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Augmentation Pipelines\n",
        "# =========================\n",
        "imagenet_mean = (0.485, 0.456, 0.406)\n",
        "imagenet_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.5),\n",
        "    A.ElasticTransform(p=0.3),\n",
        "    A.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
        "])\n",
        "\n",
        "valid_transform = A.Compose([\n",
        "    A.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
        "])\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Datasets and Loaders\n",
        "# =========================\n",
        "train_dataset = SegmentationDataset(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    augment=train_transform\n",
        ")\n",
        "\n",
        "val_dataset = SegmentationDataset(\n",
        "    X_valid,\n",
        "    y_valid,\n",
        "    augment=valid_transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n"
      ],
      "metadata": {
        "id": "IKazdKvfwoTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7a88c2-e8df-48ac-8179-541fd97c5ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\money\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# Evaluation Function\n",
        "# =====================\n",
        "def evaluate_model(model, val_loader, loss_fn, val_df_split, best_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Evaluate the model on val_loader using the best threshold.\n",
        "    Computes overall metrics and group-wise IoU/F1 by race and gender.\n",
        "\n",
        "    Args:\n",
        "        model: trained segmentation model\n",
        "        val_loader: DataLoader for validation set\n",
        "        loss_fn: loss function used\n",
        "        val_df_split: DataFrame with at least ['Patient','race','gender']\n",
        "        best_threshold: float, threshold for binarization\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    val_losses, all_preds, all_targets = [], [], []\n",
        "    patient_ids = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            val_losses.append(loss.item())\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            all_preds.append(probs.cpu())\n",
        "            all_targets.append(targets.cpu())\n",
        "\n",
        "            # Match patient IDs to batch\n",
        "            # assumes val_loader.dataset is aligned with val_df_split order\n",
        "            batch_patients = val_df_split.iloc[\n",
        "                batch_idx * val_loader.batch_size : batch_idx * val_loader.batch_size + len(inputs)\n",
        "            ][\"Patient\"].tolist()\n",
        "            patient_ids.extend(batch_patients)\n",
        "\n",
        "    # Concatenate\n",
        "    all_preds = torch.cat(all_preds, dim=0)\n",
        "    all_targets = torch.cat(all_targets, dim=0)\n",
        "\n",
        "    # Apply threshold\n",
        "    preds_bin = (all_preds > best_threshold).float()\n",
        "\n",
        "    # Overall metrics\n",
        "    stats = get_stats(preds_bin.long(), all_targets.long(), mode=\"binary\")\n",
        "    avg_iou = iou_score(*stats).mean().item()\n",
        "    avg_f1  = f1_score(*stats).mean().item()\n",
        "    avg_val_loss = np.mean(val_losses)\n",
        "\n",
        "    # --- Group metrics ---\n",
        "    results_per_patient = []\n",
        "    for i, pid in enumerate(patient_ids):\n",
        "        race   = val_df_split.loc[val_df_split[\"Patient\"] == pid, \"race\"].values[0]   if \"race\" in val_df_split.columns else \"unknown\"\n",
        "        gender = val_df_split.loc[val_df_split[\"Patient\"] == pid, \"gender\"].values[0] if \"gender\" in val_df_split.columns else \"unknown\"\n",
        "\n",
        "        y_true = all_targets[i].unsqueeze(0)   # shape [1,1,H,W]\n",
        "        y_pred = preds_bin[i].unsqueeze(0)\n",
        "\n",
        "        stats = get_stats(y_pred.long(), y_true.long(), mode=\"binary\")\n",
        "        iou = iou_score(*stats).mean().item()\n",
        "        f1  = f1_score(*stats).mean().item()\n",
        "\n",
        "        results_per_patient.append({\"Patient\": pid, \"race\": race, \"gender\": gender, \"IoU\": iou, \"Dice\": f1})\n",
        "\n",
        "    results_df = pd.DataFrame(results_per_patient)\n",
        "\n",
        "    iou_by_race  = results_df.groupby(\"race\")[\"IoU\"].mean().reset_index()\n",
        "    dice_by_race = results_df.groupby(\"race\")[\"Dice\"].mean().reset_index()\n",
        "    iou_by_gender  = results_df.groupby(\"gender\")[\"IoU\"].mean().reset_index()\n",
        "    dice_by_gender = results_df.groupby(\"gender\")[\"Dice\"].mean().reset_index()\n",
        "\n",
        "    # --- Print summary ---\n",
        "    print(f\"ðŸ“Š Overall -> Val Loss: {avg_val_loss:.4f} | IoU: {avg_iou:.4f} | F1: {avg_f1:.4f} | Threshold: {best_threshold:.2f}\")\n",
        "    print(\"ðŸ“Š IoU by Race:\\n\", iou_by_race)\n",
        "    print(\"ðŸ“Š Dice by Race:\\n\", dice_by_race)\n",
        "    print(\"ðŸ“Š IoU by Gender:\\n\", iou_by_gender)\n",
        "    print(\"ðŸ“Š Dice by Gender:\\n\", dice_by_gender)\n",
        "\n",
        "    return {\n",
        "        \"overall\": {\"val_loss\": avg_val_loss, \"iou_score\": avg_iou, \"f1_score\": avg_f1, \"threshold\": best_threshold},\n",
        "        \"per_patient\": results_df,\n",
        "        \"iou_by_race\": iou_by_race,\n",
        "        \"dice_by_race\": dice_by_race,\n",
        "        \"iou_by_gender\": iou_by_gender,\n",
        "        \"dice_by_gender\": dice_by_gender\n",
        "    }\n",
        "\n",
        "# =====================\n",
        "# Run Evaluation\n",
        "# =====================\n",
        "'''\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Build dataframe with Patient, image, mask, race, gender ---\n",
        "    #files_dir = \"/content/dataset/lgg-mri-segmentation/kaggle_3m\"\n",
        "    files_dir = \"/Users/money/Downloads/archive/kaggle_3m\"\n",
        "    file_paths = [os.path.join(dp, f) for dp, _, fn in os.walk(files_dir) for f in fn if f.endswith(\".tif\") and not f.endswith(\"_mask.tif\")]\n",
        "\n",
        "    def get_file_row(path):\n",
        "        path_no_ext, ext = os.path.splitext(path)\n",
        "        filename = os.path.basename(path)\n",
        "        patient_id = \"_\".join(filename.split(\"_\")[:3])\n",
        "        return [patient_id, path, f\"{path_no_ext}_mask{ext}\"]\n",
        "\n",
        "    val_df = pd.DataFrame((get_file_row(p) for p in file_paths), columns=[\"Patient\", \"image_filename\", \"mask_filename\"])\n",
        "\n",
        "    # Merge race/gender info\n",
        "    patient_info = pd.read_csv(os.path.join(files_dir, \"data.csv\"))\n",
        "    val_df = val_df.merge(patient_info[[\"Patient\", \"race\", \"gender\"]], on=\"Patient\", how=\"left\")\n",
        "\n",
        "    # --- Dataset & Loader ---\n",
        "    val_images = val_df[\"image_filename\"].tolist()\n",
        "    val_masks  = val_df[\"mask_filename\"].tolist()\n",
        "    val_dataset = SegmentationDataset(val_images, val_masks, augment=valid_transform)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "    # --- Load model & checkpoint ---\n",
        "    model = DeepLabV3Plus(backbone_name=\"resnet101\", encoder_weights=\"imagenet\", classes=1, activation=None)\n",
        "\n",
        "    checkpoint_path = \"/Users/money/Downloads/checkpoint_best_epoch41_iou0.7940_thr0.45.pth\"\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Found checkpoint at {checkpoint_path}\")\n",
        "    else:\n",
        "        print(f\"Checkpoint not found at {checkpoint_path}, please upload it.\")\n",
        "        #from google.colab import files\n",
        "        #uploaded = files.upload()  # Opens file picker\n",
        "        checkpoint_path = list(uploaded.keys())[0]  # Use the uploaded file\n",
        "        print(f\"âœ… Uploaded checkpoint: {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=False)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        model.eval()\n",
        "        print(f\"âœ… Loaded model from epoch {checkpoint['epoch']} with IoU={checkpoint['best_iou']:.4f}\")\n",
        "'''\n",
        "    # --- Define loss ---\n",
        "import segmentation_models_pytorch as smp\n",
        "loss_fn = smp.losses.TverskyLoss(mode=\"binary\", alpha=0.7, beta=0.3)"
      ],
      "metadata": {
        "id": "yehIk90EvuMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # --- Run evaluation ---\n",
        "    results = evaluate_model(\n",
        "        model=model,\n",
        "        val_loader=val_loader,\n",
        "        loss_fn=loss_fn,\n",
        "        val_df_split=val_df_split,\n",
        "        best_threshold= 0.5\n",
        "    )\n"
      ],
      "metadata": {
        "id": "I0HsfoIow1F2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}